# Artificial Non Intelligence - Data Analysis

## Main dependencies

- Python 3.8 or 3.9
- PostgreSQL database (currently Heroku Postgre)
- Streamlit

Create a virtual anvironemment for the project, and install the Python dependencies packages with:
```sh
pip install -r requirements.txt
```

...or, if you use Poetry (which is much better and strongly advised):
```sh
poetry install
```


## Data

JSON files from Kaggle and generated by external Notebooks are located in `/data` folder.


## To run locally

```sh
export DATABASE_URL=postgres://{user}:{password}@{hostname}:{port}/{database-name}
```

```sh
streamlit run app.py
```


## To deploy on Heroku

Add Heroku app as git origin, if necessary:
```sh
heroku git:remote -a non-intelligence-api
```

Deploy with:
```sh
git push heroku main
```

## Import data as JSON files into the database

Use the following script and edit it with the right filenames:
`python api/import_json.py`

Please note that it's recommended to first remove the existing comments from the database before importing. To do so, you can use this SQL command:
```sql
DELETE FROM comments;
```
(this should be added to the import script in the future)


## Comments crawlers

Located in /scraper.

Python script to get comments from online website comments and put them in a SQlite database. Uses a Peewee ORM and needs to be adapted to the current DB architecture.

Launch the crawler with Python from typer command file:
```sh
python3 ./scraper/crawl.py [website-crawler]
```

for example, for the crawler "Le Figaro":
```sh
python3 ./scraper/crawl.py figaro
```

